/*

Copyright (c) 2014, NVIDIA Corporation
All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this 
list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice, 
this list of conditions and the following disclaimer in the documentation 
and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND 
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED 
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. 
IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, 
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, 
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, 
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF 
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE 
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED 
OF THE POSSIBILITY OF SUCH DAMAGE.

*/

#ifndef __SYNCHRONIC
#define __SYNCHRONIC

#include <atomic>
#include <chrono>
#include <thread>

#ifdef WIN32
#include <windows.h>
#endif //WIN32

#ifdef __linux__
#include <time.h>
#include <unistd.h>
#include <linux/futex.h>
#include <sys/syscall.h>
#include <climits>
#endif //__linux__

namespace std {
namespace experimental {
inline namespace concurrency_v2 {

#ifdef __GNUC__
    //Incredible time waste to figure we need to force GCC in two places
    #define __syn_gcc_inline __attribute__((always_inline)) 
#else
    #define __syn_gcc_inline 
#endif

#ifdef __linux__
    inline void __synchronic_yield() { sched_yield(); }
#else
    inline void __synchronic_yield() { this_thread::yield(); }
#endif

#if defined(WIN32) && (defined(_M_X64) || defined(_M_IX86))
    inline void __synchronic_relax() { _mm_pause(); }
#elif defined(__linux__) && (defined(__x86_64__) || defined(__i386__))
    inline void __synchronic_relax() { asm volatile("rep; nop" ::: "memory"); }
#else
    inline void __synchronic_relax() { __synchronic_yield(); }
#endif

#if defined(_MSC_VER)
    inline constexpr bool __synchronic_expect(bool condition, int common) { return condition; }
#else
    #define __synchronic_expect __builtin_expect
#endif

#if defined(WIN32) && _WIN32_WINNT >= 0x0602

    template <class P, class V>
    void __synchronic_wait(P p, V v) {
        WaitOnAddress((PVOID)p,(PVOID)&v,sizeof(v),-1);
    }
    template <class P, class V, class T>
    void __synchronic_wait_timed(P p,V v,T t) {
        WaitOnAddress((PVOID)p,(PVOID)&v,sizeof(v),chrono::duration_cast<chrono::milliseconds>(t).count());
    }
    template <class P>
    void __synchronic_wake_one(P p) {
        WakeByAddressSingle((PVOID)p);
    }
    template <class P>
    void __synchronic_wake_all(P p) {
        WakeByAddressAll((PVOID)p);
    }
    template <class T>
    struct __synchronic_native {
        static constexpr bool value = is_pod<T>::value && (sizeof(T) <= 8);
    };

#endif

#ifdef __linux__

    template < class Rep, class Period>
    timespec __synchronic_to_timespec(chrono::duration<Rep, Period> const& delta) {
        struct timespec ts;
        ts.tv_sec = static_cast<long>(chrono::duration_cast<chrono::seconds>(delta).count());
        ts.tv_nsec = static_cast<long>(chrono::duration_cast<chrono::nanoseconds>(delta).count());
        return ts;
    }
    inline long futex(void const* addr1, int op, int val1) {
        return syscall(SYS_futex, addr1, op, val1, 0, 0, 0);
    }
    inline long futex(void const* addr1, int op, int val1, struct timespec timeout) {
        return syscall(SYS_futex, addr1, op, val1, &timeout, 0, 0);
    }
    template <class P, class V>
    void __synchronic_wait(P p, V v) {
        futex(p, FUTEX_WAIT_PRIVATE, v);
    }
    template <class P, class V, class T>
    void __synchronic_wait_timed(P p,V v,T t) {
        futex(p, FUTEX_WAIT_PRIVATE, v, __synchronic_to_timespec(t));
    }
    template <class P>
    void __synchronic_wake_one(P p) {
        futex(p, FUTEX_WAKE_PRIVATE, 1);
    }
    template <class P>
    void __synchronic_wake_all(P p) {
        futex(p, FUTEX_WAKE_PRIVATE, INT_MAX);
    }
    template <class T>
    struct __synchronic_native {
        static constexpr bool value = is_integral<T>::value && (sizeof(T) == 4);
    };

#endif

    static constexpr int __magic_number_1 = 128;
    static constexpr int __magic_number_2 = 8;
    static constexpr int __magic_number_3 = 4;

    class __synchronic_exponential_backoff {
        int microseconds = __magic_number_3;
    public:
        void sleep(int us = 0) {
            if(us != 0)
                 microseconds = us;
            this_thread::sleep_for(chrono::microseconds(microseconds));
            auto next_microseconds = microseconds + (microseconds >> 2);
            microseconds = next_microseconds < 2048 ? next_microseconds : 2048;
        }
    };

    template <class T>
    bool __synchronic_spin_for_change(const atomic<T>& arg, T nval, memory_order order) noexcept {
        for (int i = 0; i < __magic_number_1; __synchronic_relax(), ++i)
	        if (__synchronic_expect(arg.load(order) != nval, 1))
	            return true;
        for (int i = 0; i < __magic_number_2; __synchronic_yield(), ++i)
	        if (__synchronic_expect(arg.load(order) != nval, 1))
	            return true;
        __synchronic_exponential_backoff b;
        for(int i = 0; i < __magic_number_3; b.sleep(1<<i), ++i)
	        if (__synchronic_expect(arg.load(order) != nval, 1))
	            return true;
        return false;
    }

    template <class T>
    bool __synchronic_spin_for(const atomic<T>& arg, T val, memory_order order) noexcept {
        for (int i = 0; i < __magic_number_1; __synchronic_relax(), ++i)
	        if (__synchronic_expect(arg.load(order) == val, 1))
	            return true;
        for (int i = 0; i < __magic_number_2; __synchronic_yield(), ++i)
	        if (arg.load(order) == val)
	            return true;
        __synchronic_exponential_backoff b;
        for(int i = 0; i < __magic_number_3; b.sleep(1<<i), ++i)
	        if (arg.load(order) == val)
	            return true;
        return false;
    }

#if defined(__linux__) || (defined(WIN32) && _WIN32_WINNT >= 0x0602)

    template <class T>
    class __direct_futex_synchronic {
        mutable atomic<int> waiting;
        mutable atomic<int> notifying;
    public:
        __direct_futex_synchronic() : notifying(0), waiting(0) { }
        ~__direct_futex_synchronic() { 
            while(notifying.load(memory_order_acquire) != 0)
                __synchronic_spin_for(notifying, 0, memory_order_relaxed);
        }
        __syn_gcc_inline void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {

            do {
                waiting.fetch_add(1, memory_order_acq_rel);
                __synchronic_wait(&object, nval);
                waiting.fetch_add(-1, memory_order_release);
            } while(object.load(order) == nval);
        }
        template <class Rep, class Period>
        __syn_gcc_inline bool wait_for_change_for(const atomic<T>& object, T nval, 
          memory_order order, chrono::duration<Rep, Period> const& rel_time) const {

            waiting.fetch_add(1, memory_order_acq_rel);
            __synchronic_wait_timed(&object, nval, rel_time);
            waiting.fetch_add(-1, memory_order_release);
            return object.load(order) != nval;
        }
        template <class F>
        __syn_gcc_inline void notify(atomic<T>& object, F func, bool all) {

            notifying.fetch_add(1, memory_order_acq_rel);
            func(object);
            if(__synchronic_expect(waiting.fetch_add(0, memory_order_acq_rel) != 0, 0)) {
                if (all)
                    __synchronic_wake_all(&object);
                else
                    __synchronic_wake_one(&object);
            }
            notifying.fetch_add(-1, memory_order_release);
        }
    };
    template <class T>
    class __indirect_futex_synchronic : protected __direct_futex_synchronic<int> {
        typedef __direct_futex_synchronic<int> base;
        mutable atomic<int> last_writer;
    public:
        __indirect_futex_synchronic() : last_writer(-1) { }
        void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {

            last_writer.store(-1);
            while(object.load(order) == nval) {
                base::wait_for_change(last_writer, -1, memory_order_acquire);
                last_writer.store(-1);
            }
        }
        template <class Rep, class Period>
        bool wait_for_change_for(const atomic<T>& object, T nval, memory_order order, chrono::duration<Rep, Period> const& rel_time) const {

            last_writer.store(-1);
            if(object.load(order) != nval)
                return true;
            base::wait_for_change_for(last_writer, -1, memory_order_acquire, rel_time);
            return object.load(order) != nval;
        }
        template <class F>
        void notify(atomic<T>& object, F func, bool const all) {
            int me = this_thread::get_id();
            func(object);
            base::notify(last_writer, [me](auto& object) { object.store(me, memory_order_release); }, all);
        }
    };

    template <class T, class Enable = void>
    struct __synchronic_base : protected __indirect_futex_synchronic<T> {
    };

    template <class T>
    struct __synchronic_base<T, typename enable_if<__synchronic_native<T>::value>::type> : protected __direct_futex_synchronic<T> {
    };

#else

} //namespace concurrency_v2
} //namespace experimental
} //namespace std

#include <mutex>
#include <condition_variable>

namespace std {
namespace experimental {
inline namespace concurrency_v2 {

    template <class T>
    class __condition_synchronic {
        mutable atomic<int> notifying;
        mutable atomic<int> waiting;
        mutable mutex m;
        mutable condition_variable c;
    public:
        __condition_synchronic() : notifying(0), waiting(0) { }
        ~__condition_synchronic() { 
            while(notifying.load() != 0)
                __synchronic_spin_for(notifying, 0, memory_order_relaxed);
        }
        void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {
            auto guard1 = unique_lock<mutex>(m);
            waiting.fetch_add(1);
            c.wait(guard1, [&]() -> bool { return object.load(order) != nval; });
            waiting.fetch_add(-1);
        }
        template <class Rep, class Period>
        bool wait_for_change_for(const atomic<T>& object, T nval, memory_order order, chrono::duration<Rep, Period> const& rel_time) const {
            auto guard1 = unique_lock<mutex>(m);
            waiting.fetch_add(1);
            auto success = c.wait_for(guard1, [&]() -> bool { return object.load(order) != nval; }, rel_time) == cv_status::no_timeout;
            waiting.fetch_add(-1);
            return success;
        }
        template <class F>
        void notify(atomic<T>& object, F func, bool const all) {
            notifying.fetch_add(1);
            func(object);
            if (__synchronic_expect(waiting.fetch_add(0) == 0, 1))
                return;
            {
                unique_lock<mutex> guard2(m);
            }
            if (all)
                c.notify_all();
            else
                c.notify_one();
            notifying.fetch_add(-1);
        }
    };

    template <class T>
    using __synchronic_base = __condition_synchronic<T>;

#endif

    template <class T>
    class __timed_synchronic {
        using clock = conditional<chrono::high_resolution_clock::is_steady, 
                                  chrono::high_resolution_clock, chrono::steady_clock>::type;
    public:

        void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {

            __synchronic_exponential_backoff b;
            while (object.load(order) == nval)
                b.sleep();
        }
        template <class Rep, class Period>
        bool wait_for_change_for(const atomic<T>& object, T nval, memory_order order, chrono::duration<Rep, Period> const& rel_time) const {

            __synchronic_exponential_backoff b;
            auto end = clock::now() + rel_time;
            do {
                b.sleep();
                if(object.load(order) != nval)
                    return true;
            } while(clock::now() < end);
            return false;
        }
        template <class F>
        void notify(atomic<T>& object, F func, bool const) {

            func(object);
        }
    };

    // 29.9, synchronic operations

    enum class synchronic_option {
        optimize_for_short_wait,
        optimize_for_long_wait,
        optimize_for_size
    };

    template <class T, synchronic_option O = synchronic_option::optimize_for_short_wait>
    class synchronic : protected conditional<O == synchronic_option::optimize_for_size,__timed_synchronic<T>,__synchronic_base<T>>::type {
        typedef typename conditional<O == synchronic_option::optimize_for_size,__timed_synchronic<T>,__synchronic_base<T>>::type base;
    public:
        synchronic() = default;
        ~synchronic() = default;
        synchronic(const synchronic&) = delete;
        synchronic& operator=(const synchronic&) = delete;
        synchronic(synchronic&&) = delete;
        synchronic& operator=(synchronic&&) = delete;

        __syn_gcc_inline void wait(const atomic<T>& object, T desired, 
          memory_order order = memory_order_seq_cst) const {

            if (__synchronic_expect(O == synchronic_option::optimize_for_short_wait && __synchronic_spin_for(object, desired, order), 1))
                return;
            for (auto t = object.load(order); t != desired; t = object.load(order))
                base::wait_for_change(object, t, memory_order_relaxed);
        }
        __syn_gcc_inline void wait_for_change(const atomic<T>& object, T current, 
          memory_order order = memory_order_seq_cst) const {

            if (__synchronic_expect(O == synchronic_option::optimize_for_short_wait && __synchronic_spin_for_change(object, current, order), 1))
                return;
            base::wait_for_change(object, current, order);
        }

        template <class Clock, class Duration>
        __syn_gcc_inline bool wait_for_change_until(const atomic<T>& object, T current, 
          chrono::time_point<Clock, Duration> const& abs_time, 
          memory_order order = memory_order_seq_cst) const {

            return wait_for_change_for(object, current, abs_time - chrono::steady_clock::now());
        }
        template <class Rep, class Period>
        __syn_gcc_inline bool wait_for_change_for(const atomic<T>& object, T current, 
          chrono::duration<Rep, Period> const& rel_time, 
          memory_order order = memory_order_seq_cst) const {

            if (__synchronic_expect(O == synchronic_option::optimize_for_short_wait && __synchronic_spin_for_change(object, current, order), 1))
                return true;
            return base::wait_for_change_for(object, current, order, rel_time);
        }

        __syn_gcc_inline void notify_all(atomic<T>& object, T value, 
          memory_order order = memory_order_seq_cst) {

            base::notify(object, [value, order](atomic<T>& object) { object.store(value, order); }, true);
        }
        template <class F>
        __syn_gcc_inline void notify_all(atomic<T>& object, F func) {

            base::notify(object, std::forward<F>(func), true);
        }

        __syn_gcc_inline void notify_one(atomic<T>& object, T value, 
          memory_order order = memory_order_seq_cst) {

            base::notify(object, [value, order](atomic<T>& object) { object.store(value, order); }, false);
        }
        template <class F> 
        __syn_gcc_inline void notify_one(atomic<T>& object, F func) {

            base::notify(object, std::forward<F>(func), false);
        }
    };
} //namespace concurrency_v2
} //namespace experimental
} //namespace std

#endif //__SYNCHRONIC

