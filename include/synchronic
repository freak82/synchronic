/*

Copyright (c) 2014, NVIDIA Corporation
All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation
and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
OF THE POSSIBILITY OF SUCH DAMAGE.

*/

/*

ABOUT THE COMMENTS IN THIS FILE

They are specifically intended to aid in independent implementations of the
synchronic<T> type, as specified in the ISO C++ paper http://wg21.link/p0126 .

General notes:
* Each synchronic<T> type implements a polling operation with back-off.
* The synchronic<T> type definitely can have non-trivial size overheads unless the size-optimized implementation is required.
* The back-off approach always comprises four phases:
1. Poll with 'relax'. About 40 iterations.
2. Poll with 'yield'. About 40 iterations.
3. Poll with 'sleep'. About 6 iterations, with exponential delay.
4. Suspend.
* The suspension process differs greatly based on platform, and on the properties of <T>:
1. If the size-optimized implementation is used, an exponential delay is used.
2. Else, on Linux, or Windows 8+:
* For integral T of size = 4 on Linux, or else pod T of size <= 8, a futex is used directly.
* For other T, an extra integer is used to stage into a futex indirectly.
3. Elsewhere, a mutex and a condition variable are used.

It may be best to read this file from the bottom, and up.

- Olivier Giroux (ogiroux@nvidia.com)

*/

#ifndef __SYNCHRONIC
#define __SYNCHRONIC

#include <atomic>
#include <chrono>
#include <thread>
#include <cassert>

#ifdef WIN32
#include <windows.h>
#endif //WIN32

#ifdef __linux__
#include <time.h>
#include <unistd.h>
#include <linux/futex.h>
#include <sys/syscall.h>
#include <sys/types.h>
#include <climits>
#endif //__linux__

namespace std {
namespace experimental {
inline namespace concurrency_v2 {

    // On Linux, we use sched_yield and punt oversubscribed conditions to wait operations.
#ifdef __linux__
    inline void __synchronic_yield() { sched_yield(); }
#else
    // Elsewhere, we use the Standard yield for lack anything better.
    inline void __synchronic_yield() { this_thread::yield(); }
#endif

    // On x86, 'rep;nop' / 'pause' is a quick yield for hyper-threads.
#if defined(_MSC_VER)
    #if (defined(_M_X86) || defined(_M_X64))
        #define __synchronic_x86
        #define __synchronic_relax _mm_pause
    #elif defined(_M_ARM)
        #define __synchronic_arm
    #endif
#elif defined(__GNUC__) 
    #if (defined(__i386__) || defined(__x86_64__))
        #define __synchronic_x86
        //inline void __synchronic_pause() { asm volatile("pause"); }
        //#define __synchronic_relax __synchronic_pause
    #elif defined(__arm__)
        #define __synchronic_arm
    #endif
#endif

#ifndef __synchronic_relax
    #define __synchronic_relax __synchronic_yield
#endif

    // On GCC (and Clang) we provide some expected profile information.
#if defined(__GNUC__)
#define __synchronic_expect __builtin_expect
#else
    // For other compilers, we just lose this information.
#define __synchronic_expect(c,e) c
#endif

    // On Linux, we make use of the kernel memory wait operations. These have been available for a long time.
#ifdef __linux__

    // These definitions merely wrap the Linux primitives for use below.
    template < class Rep, class Period>
    timespec __synchronic_to_timespec(chrono::duration<Rep, Period> const& delta) {
        struct timespec ts;
        ts.tv_sec = static_cast<long>(chrono::duration_cast<chrono::seconds>(delta).count());
        ts.tv_nsec = static_cast<long>(chrono::duration_cast<chrono::nanoseconds>(delta).count());
        return ts;
    }
    inline void __synchronic_wait(const void* p, int v) {
        syscall(SYS_futex, p, FUTEX_WAIT_PRIVATE, v, 0, 0, 0);
    }
    template < class Rep, class Period>
    void __synchronic_wait_timed(const void* p, int v, const chrono::duration<Rep, Period>& t) {
        syscall(SYS_futex, p, FUTEX_WAIT_PRIVATE, v, __synchronic_to_timespec(t), 0, 0);
    }
    inline void __synchronic_wake_one(const void* p) {
        syscall(SYS_futex, p, FUTEX_WAKE_PRIVATE, 1, 0, 0, 0);
    }
    inline void __synchronic_wake_all(const void* p) {
        syscall(SYS_futex, p, FUTEX_WAKE_PRIVATE, INT_MAX, 0, 0, 0);
    }
    template <class T>
    struct __synchronic_native {
        static constexpr bool value = is_integral<T>::value && (sizeof(T) == 4);
    };
    // We make use of the native thread-id to extend memory wait operations to arbitrary-size locations.
    typedef pid_t __synchronic_tid;
#define __synchronic_gettid() (syscall(SYS_gettid))

#endif // __linux__

    // On Windows, we make use of the kernel memory wait operations as well. These first became available with Windows 8.
#if defined(WIN32) && _WIN32_WINNT >= 0x0602

    // These definitions merely wrap the Windows primitives for use below.
    template <class V>
    void __synchronic_wait(const void* p, V v) {
        WaitOnAddress((PVOID)p, (PVOID)&v, sizeof(v), -1);
    }
    template <class V, class Rep, class Period>
    void __synchronic_wait_timed(const void* p, V v, chrono::duration<Rep, Period> const& delta) {
        WaitOnAddress((PVOID)p, (PVOID)&v, sizeof(v), chrono::duration_cast<chrono::milliseconds>(delta).count());
    }
    inline void __synchronic_wake_one(const void* p) {
        WakeByAddressSingle((PVOID)p);
    }
    inline void __synchronic_wake_all(const void* p) {
        WakeByAddressAll((PVOID)p);
    }
    template <class T>
    struct __synchronic_native {
        // We can apply these operations to types comparable with memcmp() that are 8B or smaller.
        static constexpr bool value = is_pod<T>::value && (sizeof(T) <= 8);
    };
    // We make use of the native thread-id to extend memory wait operations to arbitrary-size locations.
    typedef DWORD __synchronic_tid;
#define __synchronic_gettid() (GetCurrentThreadId())

#endif // defined(WIN32) && _WIN32_WINNT >= 0x0602

#ifndef SYNCHRONIC_TUNE
#define __synchronic_constexpr constexpr
#else 
#define __synchronic_constexpr volatile
#endif

    // Only the order of magnitude of these numbers really matters.
#if defined(__APPLE__)
    static __synchronic_constexpr int __magic_number_1 = 64; // Poll iterations with nothing.
#elif defined(__synchronic_arm)
    static __synchronic_constexpr int __magic_number_1 = 8;
#else
    static __synchronic_constexpr int __magic_number_1 = 16;
#endif

#ifdef __synchronic_arm
    static __synchronic_constexpr int __magic_number_2 = 1; // Poll iterations with CPU support.
#else
    static __synchronic_constexpr int __magic_number_2 = 0; // Can't find any use for 'pause' on x86.
#endif

    static __synchronic_constexpr int __magic_number_3 = 64;

    static __synchronic_constexpr int __magic_number_4 = 2048;  // Maximum timed sleep during backoff, in microseconds.

    // A simple exponential back-off helper that is designed to cover the space between (1<<__magic_number_3) and __magic_number_4
    class __synchronic_exponential_backoff {
        int microseconds = __magic_number_3;
    public:
        void sleep(int us = 0) {
            if (us != 0)
                microseconds = us;
            this_thread::sleep_for(chrono::microseconds(microseconds));
            // Avoiding the use of std::min here, to keep includes minimal
            auto next_microseconds = microseconds + (microseconds >> 2);
            microseconds = next_microseconds < __magic_number_4 ? next_microseconds : __magic_number_4;
        }
    };

    // These two helper functions implement the first 3 of the 4 phases of the back-off process. They are used in more than one place.
    template <class A, class T>
    bool __synchronic_spin_for_change(const A& arg, T nval, memory_order order) noexcept {
        for (int i = 0; i < __magic_number_1; ++i)
            if (__synchronic_expect(arg.load(order) != nval, 1)) 
                return true;
#ifdef __synchronic_arm
        for (int i = 0; i < __magic_number_2; ++i) {
            T tmp;
            __asm__ __volatile__(
                "ldrex %0, [%1]\n"
                "cmp %0, %2\n"
                "it eq\n"
                "wfeeq.n\n"
                "nop.w\n"
                : "=&r" (tmp) : "r" (&arg), "r" (nval) : "cc"
                );
            if (__synchronic_expect(tmp != nval, 1)) {
                atomic_thread_fence(order);
                return true;
            }
        }
#else
        for (int i = 0; i < __magic_number_2; __synchronic_relax(), ++i)
            if (__synchronic_expect(arg.load(order) != nval, 1)) 
                return true;
#endif
        for (int i = 0; i < __magic_number_3; __synchronic_yield(), ++i)
            if (__synchronic_expect(arg.load(order) != nval, 1)) 
                return true;
#ifdef WIN32
        __synchronic_exponential_backoff b;
        for (int i = 0; i < 8; b.sleep(1 << i), ++i)
            if (__synchronic_expect(arg.load(order) != nval, 1)) 
                return true;
#endif
        return false;
    }
    template <class A, class T>
    bool __synchronic_spin_for(const A& arg, T val, memory_order order) noexcept {
        for (int i = 0; i < __magic_number_1; ++i)
            if (__synchronic_expect(arg.load(order) == val, 1))
                return true;
#ifdef __synchronic_arm
        for (int i = 0; i < __magic_number_2; ++i) {
            T tmp;
            __asm__ __volatile__(
                "ldrex %0, [%1]\n"
                "cmp %0, %2\n"
                "it ne\n"
                "wfene.n\n"
                "nop.w\n"
                : "=&r" (tmp) : "r" (&arg), "r" (val) : "cc"
                );
            if (__synchronic_expect(tmp == val, 1)) {
                atomic_thread_fence(order);
                return true;
            }
        }
#else
        for (int i = 0; i < __magic_number_2; __synchronic_relax(), ++i)
            if (__synchronic_expect(arg.load(order) == val, 1))
                return true;
#endif
        for (int i = 0; i < __magic_number_3; __synchronic_yield(), ++i)
            if (__synchronic_expect(arg.load(order) == val, 1))
                return true;
#ifdef WIN32
        __synchronic_exponential_backoff b;
        for (int i = 0; i < 3; b.sleep(1 << i), ++i)
            if (__synchronic_expect(arg.load(order) == val, 1))
                return true;
#endif
        return false;
    }

// This section defines the futex-based version of synchronic base classes.
#if defined(__linux__) || (defined(WIN32) && _WIN32_WINNT >= 0x0602)

// The macro __syn_gcc_inline applies a GCC-specific work-around to this section.
#ifdef __GNUC__

//Long-term contended critical sections are 2X worse without this. It's unclear why.
#define __syn_gcc_inline __attribute__((always_inline))
#else
#define __syn_gcc_inline
#endif

    // This class implements only the slow path, in the most optimal way. A derived class implements the fast path.
    template <class T>
    class __direct_futex_synchronic {

        mutable atomic<int> waiting;   // Used to optimize-out calls to the kernel futex wake api.
        mutable atomic<int> notifying;

#ifdef __synchronic_arm
        mutable atomic<int> notify_lock; // Used to prevent a race on destructors
        static constexpr int Unlocked = 0, Locked = 1;

        void _lock() {

            int tmp;
            __asm__ __volatile__(
                "1: ldrex %0, [%1]\n"
                "   teq %0, %2\n"
                "   itee ne\n"
                "   wfene.n\n"
                "   strexeq %0, %3, [%1]\n"
                "   teqeq %0, %2\n"
                "   bne 1b\n"
//                "   dmb\n"
                : "=&r" (tmp)
                : "r" (&this->notify_lock), "r" (Unlocked), "r" (Locked)
                : "cc"
                );
        }
        void _unlock() {

            notify_lock.store(Unlocked, memory_order_release);
            __asm__ __volatile__(
                "   dsb\n"
                "   sev"
                );
        }
#else
        mutable atomic<bool> notify_lock; // Used to prevent a race on destructors
        static constexpr bool Unlocked = false, Locked = true;

        inline void _lock() {

            while (notify_lock.load(memory_order_acquire) != Unlocked)
                __synchronic_relax();
        }
        inline void _unlock() {

            notify_lock.store(Unlocked, memory_order_release);
        }
#endif

    public:
        __direct_futex_synchronic() : waiting(0), notifying(0), notify_lock(Unlocked) { }
        ~__direct_futex_synchronic() {

            // We need to ensure that notify functions have completed or we'll have UB by use-after-free.
            _lock();
            while (notifying.load(memory_order_acquire))
                __synchronic_relax();
        }
        __syn_gcc_inline void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {

            // We reference-count the futex wait calls in order to optimize-out the futex wake calls.
            do {
                waiting.fetch_add(1, memory_order_acq_rel);
                __synchronic_wait(&object, nval);
                waiting.fetch_add(-1, memory_order_release);
            } while (object.load(order) == nval);
        }
        template <class Rep, class Period>
        __syn_gcc_inline bool wait_for_change_for(const atomic<T>& object, T nval,
            chrono::duration<Rep, Period> const& rel_time, memory_order order) const {

            // Ditto.
            waiting.fetch_add(1, memory_order_acq_rel);
            __synchronic_wait_timed(&object, nval, rel_time); // We pass the time-out directly to the kernel.
            waiting.fetch_add(-1, memory_order_release);

            // In the timed case we exploit permission to neither account for time spent polling, nor check the kernel waited long enough.
            return object.load(order) != nval;
        }
        template <class F>
        __syn_gcc_inline void notify(atomic<T>& object, F func, bool all) {

            _lock();
            try {
                func(object);
                if (__synchronic_expect(waiting.load(memory_order_acquire) != 0, 0)) {
                    notifying.fetch_add(1, memory_order_acquire);
                    _unlock();
                    if (all)
                        __synchronic_wake_all(&object);
                    else
                        __synchronic_wake_one(&object);
                    notifying.fetch_add(-1, memory_order_release);
                }
                else
                    _unlock();
            }
            catch (...) {
                _unlock();
                throw;
            }
        }
    };

    // Same as above, but for types not natively supported by the platform futex. Note the inheritance type.
    template <class T>
    class __indirect_futex_synchronic : protected __direct_futex_synchronic<__synchronic_tid> {

        // This assert is merely a reminder to the platform implementers, it is not intended to catch user bugs.
        static_assert(__synchronic_native<__synchronic_tid>::value, "Thread native handle must be a valid synchronic type for this to work.");
        typedef __direct_futex_synchronic<__synchronic_tid> base;

        // We'll use the thread id of the last writer as a proxy for the data we're watchig, on the presumption writers aren't idempotent.
        mutable atomic<__synchronic_tid> last_writer;

        static constexpr __synchronic_tid invalid_writer = -1; // I'm not sure this is correct.
    public:
        __indirect_futex_synchronic() : last_writer(invalid_writer) { }
        void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {

            // Forget about writers that precede our read.
            last_writer.store(invalid_writer);
            while (object.load(order) == nval) {

                // Wait for a new writer to arrive.
                base::wait_for_change(last_writer, invalid_writer, memory_order_acquire);
                last_writer.store(invalid_writer);
            }
        }
        template <class Rep, class Period>
        bool wait_for_change_for(const atomic<T>& object, T nval, chrono::duration<Rep, Period> const& rel_time, memory_order order) const {

            // Same as above.
            last_writer.store(invalid_writer);
            if (object.load(order) != nval)
                return true;
            base::wait_for_change_for(last_writer, invalid_writer, rel_time, memory_order_acquire);
            return object.load(order) != nval;
        }
        template <class F>
        void notify(atomic<T>& object, F func, bool const all) {

            func(object);
            base::notify(last_writer, [](auto& object) {

                // Signal that this thread wrote a new value.
                object.store(__synchronic_gettid(), memory_order_release);
            }, all);
        }
    };

    // The base class used for the slow path depends on whether it is directly supported or not.
    template <class T>
    using __synchronic_base = typename conditional<__synchronic_native<T>::value,
        __direct_futex_synchronic<T>, __indirect_futex_synchronic<T>>::type;

    // This section defines the basic version of the synchronic base classe, for when there is no platform futex.
#else

    // The macro __syn_gcc_inline does not apply in this case.
#define __syn_gcc_inline

} // namespace concurrency_v2
} // namespace experimental
} // namespace std

#include <mutex>
#include <condition_variable>

namespace std {
namespace experimental {
inline namespace concurrency_v2 {

    template <class T>
    class __condition_synchronic {

        // A mutex and condition variable approximates a futex.
        mutable condition_variable c;
        mutable mutex m;

        // We definitely still want to optimize-out unnecessary calls to the notify function.
        mutable atomic<int> waiting;
        mutable atomic<int> notifying;
        mutable atomic<bool> notify_lock;

        static constexpr bool Unlocked = false, Locked = true;

    public:
        __condition_synchronic() : waiting(0), notifying(0), notify_lock(Unlocked) { }
        ~__condition_synchronic() {

            // We need to ensure that notify functions have completed or we'll have UB by use-after-free.
            while (notify_lock.load(memory_order_acquire) != Unlocked)
                __synchronic_relax();
            while (notifying.load(memory_order_acquire))
                __synchronic_relax();
        }
        void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {
            // There is no need for a polling loop here because the condition variable includes one.
            auto guard = unique_lock<mutex>(m);
            waiting.fetch_add(+1, memory_order_relaxed);
            c.wait(guard, [&]() -> bool { return object.load(order) != nval; });
            waiting.fetch_add(-1, memory_order_relaxed);
        }
        template <class Rep, class Period>
        bool wait_for_change_for(const atomic<T>& object, T nval, chrono::duration<Rep, Period> const& rel_time, memory_order order) const {
            auto guard = unique_lock<mutex>(m);
            waiting.fetch_add(+1, memory_order_relaxed);
            auto success = c.wait_for(guard, rel_time, [&]() -> bool { return object.load(order) != nval; });
            waiting.fetch_add(-1, memory_order_relaxed);
            return success;
        }
        template <class F>
        void notify(atomic<T>& object, F func, bool const all) {


            while (notify_lock.exchange(Locked, memory_order_acquire) != Unlocked)
                __synchronic_relax();
            try {
                func(object);
                if (__synchronic_expect(waiting.load(memory_order_acquire) != 0, 0)) {
                    notifying.fetch_add(1, memory_order_acquire);
                    notify_lock.store(Unlocked, memory_order_release);
                    {
                        auto guard = unique_lock<mutex>(m);
                    }
                    if (all)
                        c.notify_all();
                    else
                        c.notify_one();
                    notifying.fetch_add(-1, memory_order_release);
                }
                else
                    notify_lock.store(Unlocked, memory_order_release);
            }
            catch (...) {
                notify_lock.store(Unlocked, memory_order_release);
                throw;
            }
        }
    };

    // This class is the only base class in this case.
    template <class T>
    using __synchronic_base = __condition_synchronic<T>;

#endif //defined(__linux__) || (defined(WIN32) && _WIN32_WINNT >= 0x0602)

    // This base class' best property is that it requires no extra storage. That is sometimes valuable.
    template <class T>
    class __timed_synchronic {

        // Choosing the best steady clock we have.
        using clock = conditional<chrono::high_resolution_clock::is_steady,
            chrono::high_resolution_clock, chrono::steady_clock>::type;
    public:

        void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {

            // Recall that this is phase 4. Some backoff already occurred in phase 3. This picks up from there automatically.
            __synchronic_exponential_backoff b;
            while (object.load(order) == nval)
                b.sleep();
        }
        template <class Rep, class Period>
        bool wait_for_change_for(const atomic<T>& object, T nval, chrono::duration<Rep, Period> const& rel_time, memory_order order) const {

            __synchronic_exponential_backoff b;
            auto end = clock::now() + rel_time;
            do {
                b.sleep();
                if (object.load(order) != nval)
                    return true;
            } while (clock::now() < end);
            return false;
        }
        template <class F>
        void notify(atomic<T>& object, F func, bool const) {

            // Another nice property of this base class is that its notify function is completely trivial.
            func(object);
        }
    };

    // 29.9, synchronic operations

    // In this implementation we now have 3 flavors of optimizations, and those are now tied to the *type*.
    enum class synchronic_type {
        optimized_for_short_wait,    // Use __synchronic_base<T> as the base class.
        optimized_for_long_wait,     // Same, but skips phases 1 through 3 of backoff.
        optimized_for_space          // Use __timed_synchronic<T> as the base class.
    };
    template <class T, synchronic_type O>
    class __synchronic_quick_base {
    public:
        static inline bool try_wait_for_change(const atomic<T>& arg, T nval, memory_order order) noexcept {
            return __synchronic_spin_for_change(arg, nval, order);
        }
        static inline bool try_wait(const atomic<T>& arg, T val, memory_order order) noexcept {
            return __synchronic_spin_for(arg, val, order);
        }
    };
    template <class T>
    class __synchronic_quick_base<T, synchronic_type::optimized_for_long_wait> {
    public:
        static inline bool try_wait_for_change(const atomic<T>&, T, memory_order) noexcept {
            return false;
        }
        static inline bool try_wait(const atomic<T>&, T, memory_order) noexcept {
            return false;
        }
    };

    template <class T, synchronic_type O>
    using __synchronic_actual_base = typename conditional<O == synchronic_type::optimized_for_space, __timed_synchronic<T>, __synchronic_base<T>>::type;

    template <class T, synchronic_type O = synchronic_type::optimized_for_short_wait>
    class synchronic : protected __synchronic_actual_base<T, O>, __synchronic_quick_base<T, O> {
        typedef __synchronic_actual_base<T, O> base;
        typedef __synchronic_quick_base<T, O> qbase;
    public:
        synchronic() = default;
        ~synchronic() = default;
        synchronic(const synchronic&) = delete;
        synchronic& operator=(const synchronic&) = delete;
        synchronic(synchronic&&) = delete;
        synchronic& operator=(synchronic&&) = delete;

        __syn_gcc_inline bool try_wait(const atomic<T>& object, T desired,
            memory_order order = memory_order_seq_cst) const {

            return qbase::try_wait(object, desired, order);
        }
        __syn_gcc_inline bool try_wait_for_change(const atomic<T>& object, T current,
            memory_order order = memory_order_seq_cst) const {

            return qbase::try_wait_for_change(object, current, order);
        }

        // All the wait functions begin with the same backoff phases 1-3. Then they call the base class for phase 4.
        __syn_gcc_inline void wait(const atomic<T>& object, T desired,
            memory_order order = memory_order_seq_cst) const {

            if (__synchronic_expect(try_wait(object, desired, order), 1))
                return;

            // Desugar into the simpler base class api for phase 4.
            for (auto t = object.load(order); t != desired; t = object.load(order))
                base::wait_for_change(object, t, memory_order_relaxed);
        }
        __syn_gcc_inline void wait_for_change(const atomic<T>& object, T current,
            memory_order order = memory_order_seq_cst) const {

            if (__synchronic_expect(try_wait_for_change(object, current, order), 1))
                return;
            base::wait_for_change(object, current, order);
        }

        template <class Clock, class Duration>
        __syn_gcc_inline bool wait_for_change_until(const atomic<T>& object, T current,
            chrono::time_point<Clock, Duration> const& abs_time,
            memory_order order = memory_order_seq_cst) const {

            // We don't implement the absolute time-outs, so we desugar into a relative time-out.
            return wait_for_change_for(object, current, abs_time - chrono::steady_clock::now(), order);
        }
        template <class Rep, class Period>
        __syn_gcc_inline bool wait_for_change_for(const atomic<T>& object, T current,
            chrono::duration<Rep, Period> const& rel_time,
            memory_order order = memory_order_seq_cst) const {

            if (__synchronic_expect(qbase::try_wait_for_change(object, current, order), 1))
                return true;
            if (rel_time <= chrono::duration<Rep, Period>::zero())
                return false;
            return base::wait_for_change_for(object, current, rel_time, order);
        }

        // All the notify functions basically convert the api into the base class api, and call that.
        __syn_gcc_inline void notify_all(atomic<T>& object, T value,
            memory_order order = memory_order_seq_cst) {

            base::notify(object, [value, order](atomic<T>& object) { object.store(value, order); }, true);
        }
        template <class F>
        __syn_gcc_inline void notify_all(atomic<T>& object, F func) {

            base::notify(object, std::forward<F>(func), true);
        }
        __syn_gcc_inline void notify_one(atomic<T>& object, T value,
            memory_order order = memory_order_seq_cst) {

            base::notify(object, [value, order](atomic<T>& object) { object.store(value, order); }, false);
        }
        template <class F>
        __syn_gcc_inline void notify_one(atomic<T>& object, F func) {

            base::notify(object, std::forward<F>(func), false);
        }
    };

} // namespace concurrency_v2
} // namespace experimental
} // namespace std

#endif // __SYNCHRONIC

