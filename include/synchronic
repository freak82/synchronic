/*

Copyright (c) 2014, NVIDIA Corporation
All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this 
list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice, 
this list of conditions and the following disclaimer in the documentation 
and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND 
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED 
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. 
IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, 
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, 
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, 
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF 
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE 
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED 
OF THE POSSIBILITY OF SUCH DAMAGE.

*/

#ifndef __SYNCHRONIC
#define __SYNCHRONIC

#include <atomic>
#include <chrono>
#include <thread>
#include <functional>
#include <algorithm>
#include <mutex>
#include <condition_variable>

//the default yield function used inside the implementation is the Standard one
#ifdef __linux__
    #define __synchronic_yield sched_yield
#else
    #define __synchronic_yield this_thread::yield
#endif

#if defined(_MSC_VER)
    #define __builtin_expect(condition,common) condition
    #define __synchronic_relax _mm_pause
#elif defined(__linux__)
    #define __synchronic_relax() asm volatile("rep; nop" ::: "memory")
#else
    #define __synchronic_relax __synchronic_yield
#endif

#if defined(_WIN32_WINNT) && _WIN32_WINNT >= 0x0602
    #include <Windows.h>
    #define __synchronic_wait(x,v) WaitOnAddress((PVOID)x,(PVOID)&v,sizeof(v),-1)
    #define __synchronic_wait_timed(x,v,t) WaitOnAddress((PVOID)x,(PVOID)&v,sizeof(v),chrono::duration_cast<chrono::milliseconds>(t).count())
    #define __synchronic_wake_one(x) WakeByAddressSingle((PVOID)x)
    #define __synchronic_wake_all(x) WakeByAddressAll((PVOID)x)
    #define __synchronic_wait_volatile(x,v) WaitOnAddress((PVOID)x,(PVOID)&v,sizeof(v),-1)
    #define __synchronic_wait_timed_volatile(x,v,t) WaitOnAddress((PVOID)x,(PVOID)&v,sizeof(v),chrono::duration_cast<chrono::milliseconds>(t).count())
    #define __synchronic_wake_one_volatile(x) WakeByAddressSingle((PVOID)x)
    #define __synchronic_wake_all_volatile(x) WakeByAddressAll((PVOID)x)
    #define __SYNCHRONIC_HAS_FUTEX(x) (is_pod<x>::value && (sizeof(x) <= 8))
#endif

#ifdef __linux__
    #include <time.h>
    #include <unistd.h>
    #include <linux/futex.h>
    #include <sys/syscall.h>
    #include <climits>
    template < class Rep, class Period>
    static inline timespec to_timespec(chrono::duration<Rep, Period> const& delta) {
        struct timespec ts;
        ts.tv_sec = static_cast<long>(chrono::duration_cast<chrono::seconds>(delta).count());
        ts.tv_nsec = static_cast<long>(chrono::duration_cast<chrono::nanoseconds>(delta).count());
        return ts;
    }
    static inline long futex(void const* addr1, int op, int val1) {
        return syscall(SYS_futex, addr1, op, val1, 0, 0, 0);
    }
    static inline long futex(void const* addr1, int op, int val1, struct timespec timeout) {
        return syscall(SYS_futex, addr1, op, val1, &timeout, 0, 0);
    }
    #define __synchronic_wait(x,v) futex(x, FUTEX_WAIT_PRIVATE, v)
    #define __synchronic_wait_timed(x,v,t) futex(x, FUTEX_WAIT_PRIVATE, v, to_timespec(t))
    #define __synchronic_wake_one(x) futex(x, FUTEX_WAKE_PRIVATE, 1)
    #define __synchronic_wake_all(x) futex(x, FUTEX_WAKE_PRIVATE, INT_MAX)
    #define __synchronic_wait_volatile(x,v) futex(x, FUTEX_WAIT, v)
    #define __synchronic_wait_volatile_timed(x,v,t) futex(x, FUTEX_WAIT, v, to_timespec(t))
    #define __synchronic_wake_one_volatile(x) futex(x, FUTEX_WAKE, 1)
    #define __synchronic_wake_all_volatile(x) futex(x, FUTEX_WAKE, INT_MAX)
    #define __SYNCHRONIC_HAS_FUTEX(x) (is_integral<x>::value && (sizeof(x) <= 4))
#endif

namespace std {
    namespace experimental {
        inline namespace concurrency_v2 {

            static constexpr int __magic_number = 42;

            template <class T>
            class __updown_guard {
                atomic<int>& object;
            public:
                __updown_guard(atomic<T>& object) noexcept : object(object) { object.fetch_add( 1, memory_order_acquire); }
                ~__updown_guard()                                           { object.fetch_add(-1, memory_order_release); }
                __updown_guard(__updown_guard&&) = default;
                __updown_guard& operator=(__updown_guard&&) = default;
                __updown_guard(const __updown_guard&) = delete;
                __updown_guard& operator=(const __updown_guard&) = delete;
            };
            template <class T>
            __updown_guard<T> __make_updown_guard(atomic<T>& object) noexcept { return __updown_guard<T>(object); }

            template <class F>
            bool __spin_for_condition(F func, int attempts) noexcept {
                int i = 0;
                for (; (attempts < 0 || i < attempts) && i < __magic_number; __synchronic_relax(), ++i)
                    if (__builtin_expect(func(), 1))
                        return true;
                for (; (attempts < 0 || i < attempts); __synchronic_yield(), ++i)
                    if (__builtin_expect(func(), 1))
                        return true;
                return false;
            }
            template <class T>
            bool __spin_for_change(const atomic<T>& arg, T nval, memory_order order, int attempts = -1) noexcept {
                return __spin_for_condition([=, &arg]() { return arg.load(order) != nval; }, attempts);
            }
            template <class T>
            bool __spin_for(const atomic<T>& arg, T val, memory_order order, int attempts = -1) noexcept {
                return __spin_for_condition([=, &arg]() { return arg.load(order) == val; }, attempts);
            }

            class __exponential_backoff {
                int microseconds = 8;
            public:
                void sleep() {
                    if (__builtin_expect(microseconds > 75, 0))
                        this_thread::sleep_for(chrono::microseconds(microseconds));
                    else if (__builtin_expect(microseconds > 25, 0))
                        __synchronic_yield();
                    else
                        __synchronic_relax();
                    microseconds = (min)(microseconds + (microseconds >> 2), 1024);
                }
            };

            template <class T>
            class __timed_synchronic {
                using clock = conditional<chrono::high_resolution_clock::is_steady, 
                                          chrono::high_resolution_clock, chrono::steady_clock>::type;
            public:
                void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {
                    __exponential_backoff b;
                    while (object.load(order) == nval)
                        b.sleep();
                }
                template <class Rep, class Period>
                void wait_for_change_for(const atomic<T>& object, T val, memory_order order, chrono::duration<Rep, Period> const& rel_time) const {
                    __exponential_backoff b;
                    chrono::milliseconds remains = then - clock::now();
                    while (object.load(order) == val && remains > chrono::milliseconds::zero()) {
                        b.sleep();
                        remains = then - clock::now();
                    }
                }
                template <class F>
                void notify(atomic<T>& object, F func, bool const) {
                    func(object);
                }
            };

            class __racy_synchronic_base {
                mutable atomic<int> expect_count = 0, notify_count = 0;
            public:
                decltype(auto) make_wait_guard() const noexcept { return __make_updown_guard(expect_count); }
                decltype(auto) make_notify_guard() const noexcept { return __make_updown_guard(notify_count); }
                bool has_wait() const noexcept { return expect_count.load() != 0; }
                ~__racy_synchronic_base() {
                    __spin_for(notify_count, 0, memory_order_acquire);
                }
            };

            template <class T>
            class __condition_synchronic : protected __racy_synchronic_base {
                mutable mutex m;
                mutable condition_variable c;
            public:
                void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {
                    auto guard = make_wait_guard();
                    unique_lock<mutex> l(m);
                    c.wait(l, [&]() -> bool { return object.load(order) != nval; });
                }
                template <class Rep, class Period>
                void wait_for_change_for(const atomic<T>& object, T val, memory_order order, chrono::duration<Rep, Period> const& rel_time) const {
                    auto guard = make_wait_guard();
                    unique_lock<mutex> l(m);
                    c.wait_for(l, [&]() -> bool { return object.load(order) != nval; }, rel_time);
                }
                template <class F>
                void notify(atomic<T>& object, F func, bool const all) {
                    auto guard = make_notify_guard();
                    func(object);
                    if (!__builtin_expect(has_wait(), 0))
                        return;
                    {
                        unique_lock<mutex> l(m);
                    }
                    if (__builtin_expect(all, 1))
                        c.notify_all();
                    else
                        c.notify_one();
                }
            };

#ifdef __SYNCHRONIC_HAS_FUTEX
            template <class T>
            class __futex_synchronic : protected __racy_synchronic_base {
            public:
                void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {
                    auto guard = make_wait_guard();
                    __synchronic_wait(&object, nval);
                }
                template <class Rep, class Period>
                void wait_for_change_for(const atomic<T>& object, T val, memory_order order, chrono::duration<Rep, Period> const& rel_time) const {
                    auto guard = make_wait_guard();
                    __synchronic_wait_timed(&object, nval, rel_time);
                }
                template <class F>
                void notify(atomic<T>& object, F func, bool const all) {
                    auto guard = make_notify_guard();
                    func(object);
                    if (!__builtin_expect(has_wait(), 0))
                        return;
                    if (__builtin_expect(all, 1))
                        __synchronic_wake_all(&object);
                    else
                        __synchronic_wake_one(&object);
                }
            };
            template <class T>
            class __indirect_futex_synchronic : protected __futex_synchronic<int> {
                atomic<int> ticket = 0, lastuser = 0;
            public:
                void wait_for_change(const atomic<T>& object, T nval, memory_order order) const {
                    auto me = ticket.fetch_add(1, memory_order_relaxed);
                    while (1) {
                        lastuser.store(me, memory_order_seq_cst);
                        if (object.load(order) != nval)
                            return;
                        __futex_synchronic<int>::wait_for_change(lastuser, me, memory_order_acquire);
                    }
                }
                template <class Rep, class Period>
                void wait_for_change_for(const atomic<T>& object, T val, memory_order order, chrono::duration<Rep, Period> const& rel_time) const {
                    auto me = ticket.fetch_add(1, memory_order_relaxed);
                    lastuser.store(me, memory_order_seq_cst);
                    if (object.load(order) != nval)
                        return;
                    __futex_synchronic<int>::wait_for_change_for(lastuser, me, memory_order_acquire, rel_time);
                }
                template <class F>
                void notify(atomic<T>& object, F func, bool const all) {
                    auto me = ticket.fetch_add(1, memory_order_relaxed);
                    func(object);
                    __futex_synchronic<int>::notify(lastuser, [=](atomic<T>& object) { object.store(me, memory_order_release); }, all);
                }
            }; 
#endif //__SYNCHRONIC_HAS_FUTEX

            template <class T, class Enable = void>
#if defined(__SYNCHRONIC_HAS_FUTEX)
            using __base_synchronic = __indirect_futex_synchronic<T>;
#elif defined(__APPLE__)
            using __base_synchronic = __condition_synchronic<T>;
#else
            using __base_synchronic = __timed_synchronic<T>;
#endif //__APPLE__

            template <class T, class Enable = void>
            struct __synchronic_base : protected __base_synchronic<T> {
            };

#ifdef __SYNCHRONIC_HAS_FUTEX
            template <class T>
            struct __synchronic_base<T, typename enable_if<__SYNCHRONIC_HAS_FUTEX(T)>::type> : protected __futex_synchronic<T> {
            };
#endif //__SYNCHRONIC_HAS_FUTEX

            // 29.9, synchronic operations

            enum class wait_hint {
                optimize_latency,
                optimize_utilization
            };

            static constexpr int __wait_attempts(wait_hint hint) {
                return hint == wait_hint::optimize_latency ? __magic_number << 1 : __magic_number >> 1;
            }

            template <class T>
            class synchronic : protected __synchronic_base<T> {
            public:
                synchronic() = default;
                ~synchronic() = default;
                synchronic(const synchronic&) = delete;
                synchronic& operator=(const synchronic&) = delete;
                synchronic(synchronic&&) = delete;
                synchronic& operator=(synchronic&&) = delete;

                void wait(const atomic<T>& object, T desired, 
                  memory_order order = memory_order_seq_cst, 
                  wait_hint hint = wait_hint::optimize_latency) const {
                    if (__builtin_expect(!__spin_for(object, desired, order, __wait_attempts(hint)), 0))
                        for (auto t = object.load(order); t != desired; t = object.load(order))
                            wait_for_change(object, t, memory_order_relaxed, hint);
                }
                void wait_for_change(const atomic<T>& object, T current, 
                  memory_order order = memory_order_seq_cst, 
                  wait_hint hint = wait_hint::optimize_latency) const {
                    if (__builtin_expect(!__spin_for_change(object, current, order, __wait_attempts(hint)), 0))
                        __synchronic_base<T>::wait_for_change(object, current, order);
                }

                template <class Clock, class Duration>
                bool wait_for_change_until(const atomic<T>& object, T current, 
                  chrono::time_point<Clock, Duration> const& abs_time, 
                  memory_order order = memory_order_seq_cst, 
                  wait_hint hint = wait_hint::optimize_latency) const {
                    wait_for_change_for(object, current, abs_time - chrono::steady_clock::now(), hint);
                }
                template <class Rep, class Period>
                bool wait_for_change_for(const atomic<T>& object, T current, 
                  chrono::duration<Rep, Period> const& rel_time, 
                  memory_order order = memory_order_seq_cst, 
                  wait_hint hint = wait_hint::optimize_latency) const {
                    if (__builtin_expect(!__spin_for_change(object, current, order, __wait_attempts(hint)), 0))
                        __synchronic_base<T>::wait_for_change_for(object, current, order, rel_time);
                }

                void notify_all(atomic<T>& object, T value, 
                  memory_order order = memory_order_seq_cst) {
                    __synchronic_base<T>::notify(object, [=, &object](atomic<T>& object) { object.store(value, order); }, true);
                }
                template <class F> void notify_all(atomic<T>& object, F func) {
                    __synchronic_base<T>::notify(object, func, true);
                }

                void notify_one(atomic<T>& object, T value, 
                  memory_order order = memory_order_seq_cst) {
                    __synchronic_base<T>::notify(object, [=,&object](atomic<T>& object) { object.store(value, order); }, false);
                }
                template <class F> void notify_one(atomic<T>& object, F func) {
                    __synchronic_base<T>::notify(object, func, false);
                }
            };
        }
    }
}

#endif //__SYNCHRONIC
